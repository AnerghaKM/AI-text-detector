{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3i20uCbPLgAp"},"outputs":[],"source":["import pandas as pd\n","from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RJ0bTZvpNDYV"},"outputs":[],"source":["def load_data():\n","    \"\"\"Load dataset from CSV file and split into training and testing sets.\"\"\"\n","\n","    dataf = pd.read_csv(r'C:\\Users\\AnerghaKMohan\\Documents\\LLM\\AIDetect\\labeled_data.csv')\n","    trains_text, tests_text, trains_labels, tests_labels = train_test_split(\n","        dataf['message'],\n","        dataf['class'],\n","        test_size=0.2\n","    )\n","\n","    return trains_text, tests_text, trains_labels, tests_labels"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":654,"status":"ok","timestamp":1710441869801,"user":{"displayName":"Anergha K M","userId":"05961624901104577362"},"user_tz":-330},"id":"YZNqkE9xNy0O","outputId":"919676bd-6438-4987-93c9-a8bfb68fe062"},"outputs":[{"data":{"text/plain":["(9541     ong I've been thinking about getting a small d...\n"," 11508    Last call challenger is a demon that runs near...\n"," 12539                 made sure of that from the age of 15\n"," 3952                     can't say the same for a car ride\n"," 18879    if he couldve only focused that interest into ...\n","                                ...                        \n"," 13621        No I just can’t put the second one above 60hz\n"," 10190               Like a pair of headphones or something\n"," 11707    i know all u youngens like to play shit games ...\n"," 11008    obviously the feds don't care about some com s...\n"," 9088                          u need to use cat fish heads\n"," Name: message, Length: 16380, dtype: object,\n"," 4398     nigga said turkish delight nothing in that shi...\n"," 6930         yeah tell me more yrx the military strategist\n"," 7378     Also why do you have a picture of me with cum ...\n"," 1021     That's rather funny. I remember reading shit l...\n"," 3942                      Perk you dont even have a permit\n","                                ...                        \n"," 10986    Hello my lawyer has forwarded me a message reg...\n"," 15499               the inside of the tree is hallowed out\n"," 29       The night was still the only sound the chirpin...\n"," 5467     and everyone with access to the score board ca...\n"," 4064     cuz ur a skid who gets banned for ddosing and ...\n"," Name: message, Length: 4096, dtype: object,\n"," 9541     0\n"," 11508    0\n"," 12539    0\n"," 3952     0\n"," 18879    0\n","         ..\n"," 13621    0\n"," 10190    0\n"," 11707    0\n"," 11008    0\n"," 9088     0\n"," Name: class, Length: 16380, dtype: int64,\n"," 4398     0\n"," 6930     0\n"," 7378     0\n"," 1021     0\n"," 3942     0\n","         ..\n"," 10986    0\n"," 15499    0\n"," 29       1\n"," 5467     0\n"," 4064     0\n"," Name: class, Length: 4096, dtype: int64)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["load_data()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-9gsqtg_N2wG"},"outputs":[],"source":["def tokenize_data(trained_text, tested_text):\n","    \"\"\"Tokenize text data using Tokenizer from Keras and pad sequences to a fixed length.\"\"\"\n","\n","    tokenizer_data = Tokenizer(\n","        num_words=5000\n","    )\n","    tokenizer_data.fit_on_texts(\n","        trained_text\n","    )\n","    trained_sequences = pad_sequences(\n","        tokenizer_data.texts_to_sequences(trained_text),\n","        maxlen=100\n","    )\n","    tested_sequences = pad_sequences(\n","        tokenizer_data.texts_to_sequences(tested_text),\n","        maxlen=100\n","    )\n","\n","    return tokenizer_data, trained_sequences, tested_sequences"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SrkWZwVtSDRa"},"outputs":[],"source":["def define_model():\n","    \"\"\"Define a convolutional neural network model using Keras.\"\"\"\n","\n","    input_layer = Input(\n","        shape=(100,)\n","    )\n","    embedding_layer = Embedding(\n","        input_dim=5000,\n","        output_dim=50\n","    )(input_layer)\n","    conv_layer = Conv1D(\n","        filters=128,\n","        kernel_size=5,\n","        activation='relu'\n","    )(embedding_layer)\n","    pooling_layer = MaxPooling1D(pool_size=5)(conv_layer)\n","    flatten_layer = Flatten()(pooling_layer)\n","    output_layer = Dense(\n","        units=1,\n","        activation='sigmoid'\n","    )(flatten_layer)\n","    model_defined = Model(\n","        inputs=input_layer,\n","        outputs=output_layer\n","    )\n","    model_defined.compile(\n","        loss='binary_crossentropy',\n","        optimizer='adam',\n","        metrics=['accuracy']\n","    )\n","\n","    return model_defined\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vigQ9CnGSHji"},"outputs":[],"source":["def train_model(model_training,sequences_training,labels_training,sequences_testing,labels_testing):\n","    \"\"\"Train the convolutional neural network model.\"\"\"\n","\n","    model_training.fit(\n","        sequences_training,\n","        labels_training,\n","        epochs=1,\n","        batch_size=1, # Works better, dont be a puss\n","        validation_data=(sequences_testing, labels_testing)\n","    )\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2AXSbvCYSKId"},"outputs":[],"source":["def save_model(saved_model):\n","    \"\"\"Save the trained convolutional neural network model to a file.\"\"\"\n","\n","    saved_model.save(\n","        'Models/ai_detection_model.h5'\n","    )"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"i09G2MUSSPrJ"},"outputs":[],"source":["def classify_input(model_classify, tokenizer_classify):\n","    \"\"\"Classify a text string input as AI or human using the trained model and tokenizer.\"\"\"\n","\n","    while True:\n","        user_input = input(\n","            'Enter a text string to classify (or \"exit\" to quit): '\n","        )\n","\n","        if user_input.lower() == 'exit':\n","            break\n","\n","        if user_input.lower() == '':\n","            continue\n","\n","        sequence = pad_sequences(\n","            tokenizer_classify.texts_to_sequences([user_input]),\n","            maxlen=100\n","        )\n","        prediction = model_classify.predict(sequence)[0][0]\n","\n","        if prediction > 0.5:\n","            print(\"AI generated\")\n","        else:\n","            print(\"Human generated\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lawiq4rASTIz","outputId":"24aea569-cdb6-4f72-b313-df4289ecb56a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m16380/16380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.0280 - val_accuracy: 0.9968 - val_loss: 0.0161\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["save model--------------\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","AI generated\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n","AI generated\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","Human generated\n"]}],"source":["if __name__ == '__main__':\n","    train_text, test_text, train_labels, test_labels = load_data()\n","    tokenizer, train_sequences, test_sequences = tokenize_data(\n","        train_text,\n","        test_text\n","    )\n","    \n","    model = define_model()\n","    \n","    train_model(model,\n","                train_sequences,\n","                train_labels,\n","                test_sequences,\n","                test_labels\n","    )\n","    save_model(\n","        model\n","    )\n","    print(\"save model--------------\")\n","    classify_input(\n","        model,\n","        tokenizer\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjT6EnBtSWEl"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPz4Cl0EOltmd7oMONYotZb","mount_file_id":"1PyN2Klj45n3dqu6l9ZI7t7Ocz_JmBDLq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
